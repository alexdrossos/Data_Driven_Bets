Due Sunday
- README with links to all parts of pipeline
- explain business problem
- overview of our pipeline
- results and analysis
- detail API capabilities and limitations

# Project 3: Data Driven Bets
# Alexandra Drossos, Courtney Smith, Blake Bormes, Austin Sanders

# Description of Problem

- Overview:

- [Data Source](https://sportsdata.io/developers/api-documentation/nfl#/fantasy/nfl-v3-projections-projected-player-game-stats-by-week-w-injuries-lineups-dfs-salaries)

- Research Questions:

# Tools Utilized

Google Cloud Platorm (GCP) - Establish Virtual Machine (VM) and infrastrucutre for
data pipeline.

Kafka - The queue for the files. Publish and consume messages for the dataset.

Zookeeper - The main node managing kafka.

Spark - Transforms the dataset to store it in a readable and queryable format.

Hadoop Distributed File System (HDFS) - Storage system for the readable tables.

Jupyter Notebook - Runs spark functions and queries; used for exploratory analysis. 

# Step-by-step Guide to Pipeline


# Links to Files

- [Project_3 Analysis Notebook]()
- [NFL API File]()
- [Spark Streaming File]()
- [Spark Batch File]()
- [Docker-Compose File]()

# Conclusion

