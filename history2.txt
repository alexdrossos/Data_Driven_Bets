#to run bash script
chmod 755 first_script
./first_script


#11/25 update
#run each of these manually for now, will need to be turned into a bash script 

docker-compose up -d
docker-compose exec cloudera hadoop fs -ls /tmp/
docker-compose exec kafka kafka-topics --create --topic games --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181
docker-compose exec -d mids env FLASK_APP=/w205/full-stack/nfl_api.py flask run --host 0.0.0.0

for i in {1..6}; do
  docker-compose exec mids \
    ab -n 1 -H "Host: user2.att.com" \
      http://localhost:5000/get_game_data
  sleep 600
done

docker-compose exec mids kafkacat -C -b kafka:29092 -t games -o beginning -e
docker-compose exec spark pyspark

#in spark
import json
import pandas as pd
from pyspark.sql.functions import explode, split
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType
import warnings
from pyspark.sql.functions import size
from pyspark.sql.functions import regexp_replace

raw_events = spark \
.read \
.format("kafka") \
.option("kafka.bootstrap.servers", "kafka:29092") \
.option("subscribe","games") \
.option("startingOffsets", "earliest") \
.option("endingOffsets", "latest") \
.load()

events = raw_events.select(raw_events.value.cast('string'))

#this will need to be modified 
final_schema = StructType([StructField('GameKey', StringType(), True),StructField('AwayTeam', StringType(), True),StructField('HomeTeam', StringType(), True)])


games_df = events.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)

#write out to HDFS
games_df.write.parquet("/tmp/games_df")


